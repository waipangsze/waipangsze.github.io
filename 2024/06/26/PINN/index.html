

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="wpsze">
  <meta name="keywords" content="">
  
    <meta name="description" content="PINN Physics-informed neural networks (PINNs) represent a significant advancement in the integration of machine learning with physical laws, particularly in the context of solving differential equatio">
<meta property="og:type" content="article">
<meta property="og:title" content="ML | Physics-informed neural networks (PINN)">
<meta property="og:url" content="https://waipangsze.github.io/2024/06/26/PINN/index.html">
<meta property="og:site_name" content="wpsze">
<meta property="og:description" content="PINN Physics-informed neural networks (PINNs) represent a significant advancement in the integration of machine learning with physical laws, particularly in the context of solving differential equatio">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.researchgate.net/profile/Zhen-Li-105/publication/335990167/figure/fig1/AS:806502679982080@1569296631121/Schematic-of-a-physics-informed-neural-network-PINN-where-the-loss-function-of-PINN_W640.jpg">
<meta property="article:published_time" content="2024-06-26T06:00:00.000Z">
<meta property="article:modified_time" content="2024-11-22T05:56:42.005Z">
<meta property="article:author" content="wpsze">
<meta property="article:tag" content="PINN">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="DeepONet">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://www.researchgate.net/profile/Zhen-Li-105/publication/335990167/figure/fig1/AS:806502679982080@1569296631121/Schematic-of-a-physics-informed-neural-network-PINN-where-the-loss-function-of-PINN_W640.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>ML | Physics-informed neural networks (PINN) - wpsze</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"waipangsze.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>wpsze</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>Links</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/bookmark/" target="_self">
                <i class="iconfont icon-bookmark"></i>
                <span>Bookmarks</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-books"></i>
                <span>Docs</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/Physics/" target="_self">
                    <i class="iconfont icon-plan"></i>
                    <span>Physics</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/Maths/" target="_self">
                    <i class="iconfont icon-plan"></i>
                    <span>Maths</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/CFD/" target="_self">
                    <i class="iconfont icon-plan"></i>
                    <span>CFD</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/ML/" target="_self">
                    <i class="iconfont icon-plan"></i>
                    <span>ML</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/Meteorology/" target="_self">
                    <i class="iconfont icon-plan"></i>
                    <span>Meteorology</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/categories/MPAS/" target="_self">
                    <i class="iconfont icon-plan"></i>
                    <span>MPAS</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/monitor/" target="_self">
                    
                    <span>Real Time Monitoring</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About Me</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://www.researchgate.net/profile/Zhen-Li-105/publication/335990167/figure/fig1/AS:806502679982080@1569296631121/Schematic-of-a-physics-informed-neural-network-PINN-where-the-loss-function-of-PINN_W640.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="ML | Physics-informed neural networks (PINN)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-06-26 14:00" pubdate>
          June 26, 2024 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.4k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          29 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">ML | Physics-informed neural networks (PINN)</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="pinn">PINN</h1>
<p>Physics-informed neural networks (PINNs) represent a significant advancement in the integration of machine learning with physical laws, particularly in the context of solving differential equations. They combine traditional neural network approaches with the constraints provided by known physics, enabling more accurate modeling and predictions in various scientific fields.</p>
<figure>
<img src="https://www.researchgate.net/profile/Zhen-Li-105/publication/335990167/figure/fig1/AS:806502679982080@1569296631121/Schematic-of-a-physics-informed-neural-network-PINN-where-the-loss-function-of-PINN_W640.jpg" srcset="/img/loading.gif" lazyload alt="PINN (source: http://dx.doi.org/10.48550/arXiv.1909.10145)" width="400" /><figcaption>PINN (source: http://dx.doi.org/10.48550/arXiv.1909.10145)</figcaption>
</figure>
<h1 id="universal-approximation-theorem">Universal Approximation Theorem</h1>
<p><strong>Definition</strong></p>
<p>The theorem states that for <strong>any continuous function</strong> $ f: ^n  $ defined on a compact subset of $ ^n $, there exists a neural network $ g $ such that the difference between $ f $ and $ g $ can be made arbitrarily small. Formally, for any $ &gt; 0 $, there exists a neural network with a finite number of neurons such that:</p>
<p><span class="math display">\[
| f(x) - g(x) | &lt; \epsilon
\]</span></p>
<p>for all $ x $ in the domain of interest.</p>
<p><strong>Historical Background</strong></p>
<p>The theorem was independently proven by George Cybenko in 1989, who focused on networks using sigmoid activation functions, and later by Kurt Hornik in 1991, who generalized it to include other activation functions like ReLU (Rectified Linear Unit).</p>
<p><strong>Implications</strong></p>
<p>It has profound implications for the design and understanding of neural networks:</p>
<ul>
<li><strong>Expressive Power</strong>: It guarantees that neural networks can model complex relationships in data, making them suitable for tasks like image classification, function approximation, and more.</li>
<li><strong>Architecture Design</strong>: The theorem informs users about the necessary architecture (e.g., number of layers and neurons) required to achieve desired approximations.</li>
</ul>
<p><strong>Limitations</strong></p>
<p>While the UAT provides a theoretical foundation, it does not address practical aspects such as:</p>
<ul>
<li><strong>Generalization</strong>: The ability of a network to perform well on unseen data is not guaranteed by the theorem alone. Techniques like regularization and cross-validation are necessary to improve generalization.</li>
<li><strong>Computational Feasibility</strong>: The theorem does not specify how to construct the network or find the optimal parameters effectively. In practice, training methods like backpropagation are used, but they may not always reach the global optimum.</li>
</ul>
<p><strong>Reading</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/">The Universal Approximation Theorem</a></li>
<li><a target="_blank" rel="noopener" href="https://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf">Expressivity and Universal Approximation Theorems Part 1</a></li>
</ul>
<h2 id="for-function-alone">For function alone,</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=42b931a30e5b8e7de7fca061d4f445ca252ac23a">Chen, T., &amp; Chen, H. (1993). Approximations of continuous functionals by neural networks with application to dynamic systems. IEEE Transactions on Neural networks, 4(6), 910-918.</a></li>
</ul>
<p>Theorem 2: Suppose that <span class="math inline">\(U\)</span> is a compact set in <span class="math inline">\(C[a,b]\)</span>, <span class="math inline">\(f\)</span> is a continuous functional defined on <span class="math inline">\(U\)</span>, and <span class="math inline">\(\sigma (x)\)</span> is a bounded generalized sigmoidal function, then for any <span class="math inline">\(\epsilon &gt; 0\)</span>, there exist <span class="math inline">\(m+1\)</span> points $ a = x_0 &lt;...&lt; x_m = b$, a positive integer <span class="math inline">\(N\)</span> and constants <span class="math inline">\(c_{ij}, \theta_{ij}, \xi_{ij}\)</span>, <span class="math inline">\(i=1,...,N\)</span>, <span class="math inline">\(j=0,1,...,m\)</span>, such that</p>
<p><span class="math display">\[
\begin{align*} 
| f(u) - \sum_{i=1}^N c_i g ( \sum_{j=0}^m \xi_{ij} u(x_{j}) + \theta_i) | &amp;&lt; \epsilon, \qquad \forall u \in U \\
| f(u) - \sum_{i=1}^N c_i g (\mathbf{\omega_j} \cdot \mathbf{x} + \theta_i) | &amp;&lt; \epsilon, \qquad \forall u \in U
\end{align*} 
\]</span></p>
<h1 id="definition-and-functionality">Definition and Functionality</h1>
<p>PINNs are a type of <strong>universal function approximator</strong> that incorporates physical laws directly into the training process of neural networks. This is achieved by embedding governing equations, typically expressed as partial differential equations (PDEs), into the loss function of the network. By doing so, PINNs ensure that the solutions generated are consistent with these physical laws, which enhances the robustness and accuracy of the model even when data availability is limited</p>
<p><strong>Training Mechanism</strong></p>
<p>The training of a PINN involves two main components:</p>
<ol type="1">
<li><strong>Data Loss</strong>: The standard supervised learning approach minimizes the mean squared error (MSE) between <strong>predicted outputs and actual data</strong>.</li>
<li><strong>Physics Loss</strong>: An additional term is introduced to the loss function that quantifies <strong>how well the predicted outputs satisfy the governing physical equations</strong>. This is done by computing gradients (<strong>using automatic differentiation</strong>) at sampled points in space and time, allowing for the evaluation of residuals from the PDEs.</li>
</ol>
<p>This dual approach allows PINNs to <strong>leverage both empirical data and theoretical knowledge</strong>, making them particularly effective for problems where data is scarce or noisy.</p>
<h2 id="challenges-and-future-directions">Challenges and Future Directions</h2>
<p>Despite their advantages, PINNs also face challenges:</p>
<ul>
<li><strong>Computational Cost</strong>: Training PINNs can be computationally intensive, especially when dealing with <strong>complex geometries</strong> or <strong>high-dimensional problems</strong>.</li>
<li><strong>Generalization Limitations</strong>: Regular PINNs typically require <strong>retraining for different geometries or boundary conditions</strong>, which can limit their efficiency in practical applications.</li>
</ul>
<h1 id="pinn-1">PINN</h1>
<h2 id="key-points-on-using-collocation-points-in-pinns">Key Points on Using Collocation Points in PINNs</h2>
<figure>
<img src="https://www.researchgate.net/publication/354493654/figure/fig2/AS:1066347421261825@1631248445040/Distributions-of-collocation-points-t-f-x-f-for-the-evaluation-of-governing.png" srcset="/img/loading.gif" lazyload alt="Distributions of collocation points (t f , x f ) for the evaluation of governing equations (a) logarithmically sampled time points, and (b) uniformly sampled time points. The data sets (t o , x o ) and (t b , x b ) are also plotted." width="550" /><figcaption>Distributions of collocation points (t f , x f ) for the evaluation of governing equations (a) logarithmically sampled time points, and (b) uniformly sampled time points. The data sets (t o , x o ) and (t b , x b ) are also plotted.</figcaption>
</figure>
<p>It is possible to use collocation points only in Physics-Informed Neural Networks (PINNs). Collocation points are specific locations in the domain where the residuals of the governing equations are evaluated, and they play a crucial role in training PINNs.</p>
<div class="note note-primary">
            <p>Neural networks can indeed be utilized to approximate functions with undetermined parameters, and <strong>one of their key advantages is the ability to compute various operators, including derivatives and partial derivatives, directly on these networks</strong>. This capability is particularly beneficial in applications like Physics-Informed Neural Networks (PINNs), where the derivatives of the network output with respect to inputs are essential for enforcing physical laws.</p><p>神經網路確實可以用來近似具有未確定參數的函數，其主要優點之一是能夠<strong>直接在這些神經網路上計算各種運算符，包括導數和偏導數</strong>。此功能在物理資訊神經網路 (PINN) 等應用中特別有用。</p>
          </div>
<p><strong>1. Definition and Role</strong></p>
<p>Collocation points serve as the training dataset for PINNs, where the network learns to <strong>minimize the residuals of the partial differential equations (PDEs) at these discrete points</strong>. The objective is to ensure that the neural network's output satisfies the physical laws represented by these equations at the selected collocation points.</p>
<p><strong>2. Training with Collocation Points</strong></p>
<p>The training process involves minimizing a loss function that combines:</p>
<ul>
<li><strong>Data Loss</strong>: Measures how well the model fits any <strong>available observational data</strong>.</li>
<li><strong>Physics Loss</strong>: Enforces that the residuals of the PDEs at the <strong>collocation points</strong> are minimized.</li>
</ul>
<p>Using only collocation points can be effective, especially when there is limited observational data. However, it is essential to select these points wisely, as their distribution can significantly affect the accuracy and convergence of the solution. <strong>Strategies such as adaptive selection of collocation points</strong> based on residual analysis or problem-specific features can enhance performance.</p>
<p><strong>3. Limitations and Considerations</strong></p>
<p>While using collocation points alone is feasible, there are considerations:</p>
<ul>
<li><strong>Distribution of Points</strong>: A uniform distribution may not capture complex behaviors in certain regions of the domain effectively. <strong>Adaptive methods that focus on areas with higher residuals or complexity</strong> can improve training efficiency and solution accuracy.</li>
<li><strong>Generalization</strong>: Relying solely on collocation points may limit the model's ability to generalize across different scenarios or boundary conditions unless carefully managed.</li>
</ul>
<p>In summary, using only collocation points in PINNs is a valid approach, but careful consideration of their selection and distribution is crucial for achieving accurate and reliable results.</p>
<h2 id="strategies-for-selecting-collocation-points">Strategies for Selecting Collocation Points</h2>
<p><strong>1. Uniform Sampling</strong></p>
<ul>
<li><strong>Description</strong>: Initially, many PINNs utilize a uniform sampling strategy, distributing collocation points evenly across the domain.</li>
<li><strong>Limitations</strong>: This method may lead to inefficiencies, especially in complex problems where certain regions require more focus due to higher gradients or residuals.</li>
</ul>
<p><strong>2. Adaptive Sampling Techniques</strong></p>
<ul>
<li><strong>Dynamic Adjustment</strong>: Adaptive strategies adjust the locations of collocation points during training based on the residuals of the PDEs. This allows the network to concentrate on areas where the solution is more complex or where errors are larger.</li>
<li><strong>Examples</strong>:
<ul>
<li><strong>PINNACLE Method</strong>: This method dynamically selects collocation and experimental points by analyzing training dynamics and adjusting point locations based on performance metrics. It improves accuracy and convergence speed by focusing on regions with significant residuals.</li>
<li><strong>Residual-Based Sampling</strong>: Points are selected based on the distribution of residuals, with higher densities in areas exhibiting larger errors. This method can involve <strong>resampling after a set number of iterations to avoid local minima</strong>.</li>
</ul></li>
</ul>
<p><strong>3. Weighting Collocation Points</strong></p>
<ul>
<li><strong>Variable Influence</strong>: Assign different weights to collocation points based on their importance, allowing the model to focus more on points with higher loss values. This approach helps prioritize learning in critical areas of the solution space.</li>
</ul>
<p><strong>4. Probability Density Functions (PDFs)</strong></p>
<ul>
<li><strong>Guided Distribution</strong>: Use information from residuals to create a probability density function that guides the distribution of collocation points. This can lead to a non-uniform distribution that better captures the essential features of the solution.</li>
</ul>
<p><strong>5. Hybrid Approaches</strong></p>
<ul>
<li>Combining fixed and adaptive methods can also be beneficial. For instance, starting with uniformly distributed points and then adapting their locations based on observed performance can provide a balanced approach.</li>
</ul>
<p>In conclusion, <strong>selecting collocation points in PINNs is an iterative process</strong> that can significantly impact the model's efficiency and accuracy. Adaptive methods that focus on error distribution and dynamic adjustments during training are particularly effective, as demonstrated by recent advancements like the PINNACLE method. These strategies allow for better representation of complex physical phenomena while optimizing computational resources.</p>
<h1 id="video">Video</h1>
<ul>
<li>Steve Brunton</li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-zrY7P2dVC4?si=u64XUih1hSEUEv6o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
</iframe>
<h1 id="physics">Physics</h1>
<ol type="1">
<li>Baty, H. (2024). A hands-on introduction to Physics-Informed Neural Networks for solving partial differential equations with benchmark tests taken from astrophysics and plasma physics. arXiv preprint arXiv:2403.00599.
<ol type="1">
<li>(Left panel) Distribution of data sets showing the space localization of training data <strong>points at the four boundaries</strong> (i.e. <span class="math inline">\(N_{data} = 120\)</span>) and <strong>collocation points</strong> (i.e. <span class="math inline">\(N_c = 400\)</span>) inside the domain, for solving Laplace problem using vanilla-PINNs. (Right panel) Evolution of the two partial losses <span class="math inline">\(L_{data}\)</span> and $L_{PDE} $ as functions of the number of iterations (i.e. epochs).</li>
</ol></li>
</ol>
<div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://arxiv.org/html/2403.00599v1/extracted/5443010/figur6.png" srcset="/img/loading.gif" lazyload /></div><div class="group-image-wrap"><img src="https://arxiv.org/html/2403.00599v1/extracted/5443010/figur6b.png" srcset="/img/loading.gif" lazyload /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://arxiv.org/html/2403.00599v1/extracted/5443010/figur7.png" srcset="/img/loading.gif" lazyload /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://arxiv.org/html/2403.00599v1/extracted/5443010/figur9a.png" srcset="/img/loading.gif" lazyload /></div><div class="group-image-wrap"><img src="https://arxiv.org/html/2403.00599v1/extracted/5443010/figur9b.png" srcset="/img/loading.gif" lazyload /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://arxiv.org/html/2403.00599v1/extracted/5443010/figur10.png" srcset="/img/loading.gif" lazyload /></div></div></div>
<ol start="2" type="1">
<li>Schiassi, Enrico, et al. "Physics-Informed Neural Networks for Optimal Planar Orbit Transfers." Journal of Spacecraft and Rockets (2022): 1-16.</li>
<li>Barreau, Matthieu, et al. "Physics-informed learning for identification and state reconstruction of traffic density." arXiv preprint arXiv:2103.13852 (2021).</li>
</ol>
<h1 id="blog">Blog</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662739405?utm_psn=1832573966142803968">浅谈一下PINN(物理启发神经网络）西南望长安</a>
<ul>
<li>PINN的基本原理起源于2017年Prof.George Karniadarkis 组Marziar Raissi的工作 (2017年已经放在了Arxiv上)
<ul>
<li>Physics Informed Deep Learning:Data-driven Solutions of Nonlinear Partial Differential Equations</li>
</ul></li>
<li>分别用于解决反问题（inverse problem）和正问题（forward problem）
<ul>
<li>正问题和反问题的正经定义可以解释为：正问题，已知原因，根据已有的模型和规律，得到结果状态或者观测，而反问题则是已知结果状态或者观测，来反推原因。</li>
<li>反问题我们定义为，已知一些场域内的观测情况，来反推最优的PDE方程的系数/参数的值（这个定义是非常狭隘的和最基本的，后面我们再延申），从某种角度来开，正问题和反问题是一套系统的共生问题</li>
</ul></li>
<li>神经网络的可微性带来了梯度求解的可行性，而现有的pytorch等框架的autograd带来了工程上实现的便利性</li>
<li>正是由于<strong>PDE方程已经包含了所有信息(一个世界模型)</strong>，因此，求解正问题时，<strong>PINN完全不需要数据，只需要随意在空间和时间步上采样，然后让PDE方程来评估神经网络的建模是否准确</strong></li>
</ul></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/468748367?utm_psn=1832689886811004928">​内嵌物理知识神经网络（PINN）是个坑吗？ zwqwo</a>
<ul>
<li>PINN的原理就是通过训练神经网络来最小化损失函数来近似PDE的求解，所谓的损失函数项包括初始和边界条件的残差项，以及<strong>区域中选定点（按传统应该称为"配点"）处的偏微分方程残差</strong>。训练完成后进行推断（Inference）就可以得到时空点上的值了。</li>
<li>特别是用于解决与偏微分方程 （PDE） 相关的各种问题，包括方程求解、参数反演、模型发现、控制与优化等。</li>
<li>相比传统微分方程数值求解的描述，这里多出了行式子，也就是对于数据的使用，这也是PINN的特点之一。当然，这三个条件也不一定全都出现，比如<strong>边界条件消失，从传统数值方法的角度来看甚至不能满足定解条件</strong>。</li>
<li>自动微分</li>
<li>但PINN这种神经网络不行，即使对于线性方程，也不得不使用<strong>非线性求解器</strong>（迭代优化器），比如L-BFGS，或者神经网络训练中用得更多的SGD、Adam等。非线性问题的求解通常比线性问题难，这是PINN计算效率上一个避不开的障碍。</li>
<li>从这么看来，PINN确实不是太过于新奇的东西。至少在<em>1994年的文献中已经有使用MLP求解二维Poisson方程</em>的例子：
<ul>
<li>Dissanayake, M. W. M. G., and Nhan Phan‐Thien. "Neural‐network‐based approximations for solving partial differential equations." communications in Numerical Methods in Engineering 10.3 (1994): 195-201.</li>
</ul></li>
<li>小结一下以上内容
<ul>
<li>大家都知道PINN是一种（深度）网络，在定义时空区域中给定一个输入点，在训练后在微分方程的该点中产生估计的解。</li>
<li>结合对控制方程的嵌入得到残差，利用<strong>残差构造损失项</strong>就是PINN的一项不太新奇的新奇之处了。本质原理就是将方程（也就是所谓的物理知识）集成到网络中，并使用来自控制方程的残差项来构造损失函数，由该项作为惩罚项来限制可行解的空间。</li>
<li><strong>用PINN来求解方程并不需要有标签的数据，比如先前模拟或实验的结果</strong>。从这个角度，对PINN 在深度学习中的地位进行定位的话，大概是处于无监督、自监督、半监督或者弱监督的地位，这几个不尽相同的说法在不同语境下都有文献提过。</li>
<li><strong>PINN算法本质上是一种无网格技术，通过将直接求解控制方程的问题转换为损失函数的优化问题来找到偏微分方程解</strong>。</li>
</ul></li>
<li>如果把PINN当作是单纯的数值求解器，通常来讲，不管从速度或者精度，PINN在性能上并不能跟传统方法（有限差分、有限元、有限体积等大类方法）抗衡。(not sure)</li>
<li>“内嵌物理”（Physics informed）对于科学机器学习的必要性。值得一提的是，内嵌物理知识神经网络并不是一种作为数据驱动方法对立面的、单纯的“知识驱动”方法，而是作为数据方法与传统知识驱动方程的桥梁存在，也就是“知识”和“数据”共同驱动的方法</li>
<li>这节对一个不是太简单的方程进行了数值求解，<strong>但解方程其实也并不是PINN的主业</strong>。当然，<strong>对于PINN而言，解方程这种“正问题”跟参数发现等这些“逆问题”在求解形式上没有太大区别</strong>，能比较好求解“正问题”的PINN方法对于“逆问题”也会有不错的表现。</li>
<li>CFD与其说是计算科学，更像是一门实验科学，需要物理实验数据来对解算的模型进行验证，还面临诸多困难。
<ul>
<li>在工程模型中，可能还涉及逆问题的求解，<strong>也就是边界条件和流体的各种参数未知的情形下，如何通过部分测量数据得到精确的模型参数和流场的重构</strong>。</li>
<li>CFD网格质量对结果的影响比较大，计算中网格划分本身也是非常耗时的。最后，目前的CFD软件都非常庞大，比如OpenFoam，对每一类问题都有专门的求解模块，拥有超过10万行的科学计算代码，其更新与维护也是一件难事。</li>
<li>刚好，PINN有部分解决以上问题的能力。<strong>PINN对各种数据的融合是非常自然的</strong>，不管是压强标量场的时空散点数据、速度矢量场的时空散点数据，还是示踪粒子的运动轨迹，都可以非常容易地融合到PINN的求解中。</li>
<li><strong>对于PINN来讲，求解正问题与求解包含数据的逆问题在形式上并没有太大区别</strong>，这是它的巨大优势之一。</li>
<li>即在时空散点测量数据比较充足的情况下，进行CFD相关的<strong>参数估计、流场重建、代理模型构建</strong>等问题的求解。与传统的CFD求解器相比，PINN在集成数据（流量的观测值）和物理知识（其实就是描述该物理现象的控制方程）方面更胜一筹。</li>
</ul></li>
<li>比较直接的是<strong>通过速度观测来重建全流场</strong>。在气动力学等学科的实验研究中，可以利用光学设备，通过粒子图像测速（Particle Image Velocimetry，PIV）和Particle Tracking Velocimetry（PTV）方法测量的得到多个散点速度。然而散点速度并不能满足需求，高分辨率的速度场对于可视化和后续分析是必不可少的。一个非常自然的想法就是通过类似图像插值来实现从散点到高分辨率流场的“超分辨”，但是这种方式处理得到的结果可能“并不符合物理规律”。作为融合了物理知识的PINN方法，可以非常自然地从这些稀疏速度信息来重建分辨率的整体速度场。</li>
<li><strong>连初值和边界条件都不需要。这种适合PINN建模求解的逆问题对于传统的CFD求解器来说并不是一件容易的事情</strong>。</li>
<li><strong>PINN的优势不再正问题求解上的速度和精度，而是在于融合数据和知识</strong>。</li>
</ul></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/526459912/answer/2776226543">PINN还有研究的必要吗？AI4Science探索</a>
<ul>
<li>PINN即内嵌物理知识神经网络，该领域更广泛、通用叫法应该是物理驱动的神经网络(深度学习)，刚接触到物理驱动的神经学习方法时，总会有一些疑惑：物理驱动的深度学习方法在求解一些物理系统（由物理方程所描述控制的系统）时，需要已知一些物理信息如偏微分方程。<strong>但传统数值方法发展这么多年了，如有限差分、有限体积方法已经非常成熟，也成功用于物理系统的求解，求解准确性非常高</strong>。但是物理驱动的深度学习如神经网络方法</li>
<li>相比于传统数值方法有以下潜在优势：
<ul>
<li><strong>反问题计算上有比较大的优势</strong>。传统数值方法主要针对复杂问题的正计算，如已知边界条件、已知控制方程下的正计算，优势非常强。相比之下，在正计算问题上，深度学习方法逊色一些。但针对一些反问题，如已知一些测量数据和部分物理 <strong>(方程中某些参数未知、边界条件未知)</strong>，深度学习方法可以形成数据和物理双驱动的模型，比基于传统数值方法去做数据同化（data assimilation）效率更高。</li>
<li><strong>需要做快速推断时</strong>，优势更明显。一方面，当面对一些数值问题，可以不需要熟悉数值方法背景（不必利用数值格式去推导求解），可以直接利用加物理损失的方法得到一个参考解；另一方面，当问题边界需要不停地换，或者很多源需要不停的变化的问题设定下，如果利用大量时间去训练一个网络，如利用lulu老师提出<strong>deeponet方法</strong>（物理驱动的神经网络方法），在推断阶段就能实现快速预测。高维问题上的潜在优势。</li>
<li>神经网络确实能处理许多高维问题，但很难说神经网络方法在一些benchmark问题上已经完全超越了传统问题，还需要进一步讨论。</li>
</ul></li>
</ul></li>
</ul>
<h1 id="governing-equations-from-scarce-data">governing equations from scarce data</h1>
<ul>
<li>Chen, Z., Liu, Y., &amp; Sun, H. (2021). Physics-informed learning of governing equations from scarce data. Nature communications, 12(1), 6136.
<ul>
<li>通过数据发现描述复杂物理系统的控制方程或规律，可以极大地促进各个科学和工程领域对这些系统的建模、仿真和理解。本文提出了一种新颖的方法，<strong>称为稀疏回归的物理启发神经网络</strong>，用于从稀疏和噪声数据中<strong>发现非线性时空系统的控制偏微分方程</strong>。特别地，该发现方法无缝集成了深度神经网络在丰富表示学习、物理嵌入、自动微分和稀疏回归方面的优势，以逼近系统变量的解，计算必要的导数，以及识别形成方程结构和显式表达式的关键导数项和参数。该方法的有效性和鲁棒性在多种偏微分方程系统的数值和实验上得到了验证，这些系统考虑了不同的数据稀缺性和噪声水平，以及不同的初始/边界条件。所得的计算框架<strong>显示了在大规模准确数据难以获取的实际应用中发现封闭形式模型的潜力</strong>。</li>
</ul></li>
</ul>
<figure>
<img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41467-021-26434-1/MediaObjects/41467_2021_26434_Fig4_HTML.png?as=webp" srcset="/img/loading.gif" lazyload alt="Fig. 4: Discovered Fitzhugh–Nagumo equations based on data sampled under three initial conditions (ICs) with 10% noise." /><figcaption>Fig. 4: Discovered Fitzhugh–Nagumo equations based on data sampled under three initial conditions (ICs) with 10% noise.</figcaption>
</figure>
<h1 id="generalization-of-pinns">Generalization of PINNs</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://elib.dlr.de/185457/1/master_thesis.pdf#page=36.10">Schäfer, Violetta. Generalization of physics-informed neural networks for various boundary and initial conditions. Diss. Technische Universität Kaiserslautern, 2022. (Master Thesis)</a></li>
<li></li>
</ul>
<h1 id="references">References</h1>
<ol type="1">
<li>Dissanayake, M. W. M. G., and Nhan Phan‐Thien. "Neural‐network‐based approximations for solving partial differential equations." communications in Numerical Methods in Engineering 10.3 (1994): 195-201.</li>
<li>Raissi, Maziar, Paris Perdikaris, and George Em Karniadakis. "Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations." arXiv preprint arXiv:1711.10561 (2017).</li>
<li>Raissi, M., Perdikaris, P., &amp; Karniadakis, G.E. (2017). Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations. ArXiv, abs/1711.10566.</li>
<li>Raissi, M., Perdikaris, P., &amp; Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378, 686-707.</li>
<li>Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., &amp; Yang, L. (2021). Physics-informed machine learning. Nature Reviews Physics, 3(6), 422-440.</li>
<li>Lu, L., Jin, P., Pang, G., Zhang, Z., &amp; Karniadakis, G. E. (2021). Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature machine intelligence, 3(3), 218-229.</li>
<li>Lu, Lu, et al. "DeepXDE: A deep learning library for solving differential equations." SIAM Review 63.1 (2021): 208-228.</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/ML/" class="category-chain-item">ML</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/PINN/" class="print-no-link">#PINN</a>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
        <a href="/tags/DeepONet/" class="print-no-link">#DeepONet</a>
      
        <a href="/tags/ML/" class="print-no-link">#ML</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>ML | Physics-informed neural networks (PINN)</div>
      <div>https://waipangsze.github.io/2024/06/26/PINN/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>wpsze</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>June 26, 2024</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Updated on</div>
          <div>November 22, 2024</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/07/04/ssh_tunnel/" title="ssh tunnel">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">ssh tunnel</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/06/25/PINN-JAX/" title="ML | JAX">
                        <span class="hidden-mobile">ML | JAX</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.utils.listenDOMLoaded(function() {
      Fluid.events.registerRefreshCallback(function() {
        if ('mermaid' in window) {
          mermaid.init();
        }
      });
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">

  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  

  <span id="sitetime"></span>
  <script language=javascript>
    function siteTime(){
      window.setTimeout("siteTime()", 1000);
      var seconds = 1000;
      var minutes = seconds * 60;
      var hours = minutes * 60;
      var days = hours * 24;
      var years = days * 365;
      var today = new Date();
      var todayYear = today.getFullYear();
      var todayMonth = today.getMonth()+1;
      var todayDate = today.getDate();
      var todayHour = today.getHours();
      var todayMinute = today.getMinutes();
      var todaySecond = today.getSeconds();
      /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
      year - 作为date对象的年份，为4位年份值
      month - 0-11之间的整数，做为date对象的月份
      day - 1-31之间的整数，做为date对象的天数
      hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
      minutes - 0-59之间的整数，做为date对象的分钟数
      seconds - 0-59之间的整数，做为date对象的秒数
      microseconds - 0-999之间的整数，做为date对象的毫秒数 */
      var t1 = Date.UTC(2023,04,23,00,00,00); //北京时间2018-2-13 00:00:00
      var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
      var diff = t2-t1;
      var diffYears = Math.floor(diff/years);
      var diffDays = Math.floor((diff/days)-diffYears*365);
      var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
      var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
      var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
      /*document.getElementById("sitetime").innerHTML=" 已运行"+/*diffYears+" 年 "+*/diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
      document.getElementById("sitetime").innerHTML=" 已運行"+diffYears+" 年 "+diffDays+" 天 ";
    }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
    siteTime();
  </script>
  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
