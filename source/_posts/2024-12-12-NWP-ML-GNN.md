---
layout: post
title: NWP | ML | GNN Graph Neural Network Model
categories: [ML]
tags: [ML, AI, pytorch, GNN, ECMWF]
author: wpsze
date: 2024-12-12 14:55:00
math: true
mathjax: true
mathjax_autoNumber: true
mermaid: true
index_img: https://i.imgur.com/SU2M541.png
banner_img: https://i.imgur.com/SU2M541.png
---

# Overview of Graph Neural Networks (GNNs)

Graph Neural Networks (GNNs) are a specialized type of neural network designed to process and analyze graph-structured data. Unlike traditional neural networks that operate on structured data (like images or text), GNNs can directly handle the complex relationships represented in graphs, making them particularly useful for tasks where data is interconnected.

## What is a Graph?

A graph is a mathematical structure consisting of **nodes** (or vertices) and **edges** (connections between nodes). Formally, a graph $ G $ can be defined as $ G = (V, E) $, where $ V $ is the set of nodes and $ E $ is the set of edges. Graphs can represent various types of data, such as social networks, molecular structures, and transportation systems.

## How GNNs Work

GNNs utilize a process called **message passing**, where information is exchanged between neighboring nodes. Each node updates its state by aggregating information from its neighbors, allowing the network to learn representations that capture the structure and features of the graph. The final output for each node is known as its **embedding**, which encapsulates the node's information in relation to its neighbors. (Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph.)

## Types of GNNs

There are several variations of GNNs, each suited for different applications:

- **Graph Convolutional Networks (GCNs)**: These networks extend convolutional neural networks (CNNs) to graph data by learning features from neighboring nodes through convolution-like operations.
- **Recurrent Graph Neural Networks (RGNNs)**: RGNNs are designed for handling temporal or sequential data on graphs, leveraging recurrent mechanisms to model dependencies over time.
- **Gated Graph Neural Networks (GGNNs)**: These networks introduce gating mechanisms to better manage long-range dependencies within graph structures.
- **Graph Autoencoders**: Used primarily for unsupervised learning tasks such as link prediction, these models encode graph data into lower-dimensional representations and then attempt to reconstruct the original graph.

## Applications of GNNs

GNNs are versatile and can be applied in various domains:

- **Node Classification**: Predicting labels for nodes in a graph, often used in social networks or citation networks.
- **Link Prediction**: Estimating the likelihood of connections between nodes, useful in recommendation systems.
- **Graph Classification**: Classifying entire graphs based on their structure and node features, applicable in molecular chemistry and bioinformatics.

![](https://i.imgur.com/9cs4ilH.png){width=600}

## Conclusion

Graph Neural Networks represent a significant advancement in machine learning by enabling the analysis of complex relational data. Their ability to learn from the structure and features of graphs opens up new possibilities across numerous fields, from social sciences to biology and beyond. As research continues to evolve, GNNs are likely to become increasingly integral to understanding and leveraging interconnected data.

# [A Gentle Introduction to Graph Neural Networks (**Recommend**)](https://distill.pub/2021/gnn-intro/)

{% note danger %}
A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries (permutation invariances). 
{% endnote %}

We’re going to build GNNs using the “message passing neural network” framework proposed by [Gilmer et al.](https://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf) using the Graph Nets architecture schematics introduced by [Battaglia et al.](https://www.researchgate.net/profile/Andrea-Tacchetti/publication/325557043_Relational_inductive_biases_deep_learning_and_graph_networks/links/5cd0ad38299bf14d957cca5c/Relational-inductive-biases-deep-learning-and-graph-networks.pdf)

GNNs adopt a **“graph-in, graph-out” architecture** meaning that these model types accept a graph as input, with information loaded into its nodes, edges and global-context, and progressively transform these embeddings, without changing the connectivity of the input graph.

- What types of problems have graph structured data?
  - We have described some examples of graphs in the wild, but what tasks do we want to perform on this data? There are three general types of prediction tasks on graphs: **graph-level, node-level, and edge-level**.
    - In a `graph-level task`, we predict a single property for a whole graph. 
    - For a `node-level task`, we predict some property for each node in a graph.
    - For an `edge-level task`, we want to predict the property or presence of edges in a graph.
  - For the three levels of prediction problems described above (graph-level, node-level, and edge-level), we will show that all of the following problems can be solved with a single model class, the GNN. But first, let’s take a tour through the three classes of graph prediction problems in more detail, and provide concrete examples of each.

![](https://i.imgur.com/lTzKFu6.png){width=600}
![](https://i.imgur.com/QzUIet1.png){width=600}

However, it is not always so simple. For instance, you might have information in the graph stored in edges, but no information in nodes, but still need to make predictions on nodes. We need a way to collect information from edges and give them to nodes for prediction. We can do this by pooling. `Pooling proceeds` in two steps:

1. For each item to be pooled, gather each of their embeddings and concatenate them into a matrix.
2. The gathered embeddings are then aggregated, usually via a sum operation.

We represent the **pooling operation** by the letter $\rho$, and denote that we are gathering information from edges to nodes as $p_{E_n \to V_{n}}$.

![](https://i.imgur.com/IuZEdeO.png){width=600}

![](https://i.imgur.com/SU2M541.png){width=600}


## Other types of graphs (multigraphs, hypergraphs, hypernodes, hierarchical graphs)

We can also consider nested graphs, where for example a node represents a graph, also called a [hypernode graph](https://dl.acm.org/doi/pdf/10.1145/174608.174610). Nested graphs are useful for **representing hierarchical information**. For example, we can consider a network of molecules, where a node represents a molecule and an edge is shared between two molecules if we have a way (reaction) of transforming one to the other [(1)](https://academic.oup.com/bioinformatics/article-pdf/34/13/i457/50316205/bioinformatics_34_13_i457.pdf), [(2)](https://www.nature.com/articles/s41467-020-19267-x.pdf). In this case, we can learn on a nested graph by having a GNN that learns representations at the molecule level and another at the reaction network level, and alternate between them during training.

![](https://i.imgur.com/36EVPAi.png){width=600}

# Regional data-driven weather modeling with a global stretched-grid

- [Nipen, Thomas Nils, et al. "Regional data-driven weather modeling with a global stretched-grid." arXiv preprint arXiv:2409.02891 (2024).](https://arxiv.org/pdf/2409.02891)

![](https://i.imgur.com/4C4jXs3.png){width=600}
![](https://i.imgur.com/KIhFwXW.png){width=600}
![](https://i.imgur.com/cG9STZy.png){width=600}

# References

1. [A Gentle Introduction to Graph Neural Networks (**Recommend**)](https://distill.pub/2021/gnn-intro/)
2. [A Comprehensive Introduction to Graph Neural Networks (GNNs)](https://www.datacamp.com/tutorial/comprehensive-introduction-graph-neural-networks-gnns-tutorial)
3. [Introduction to Graph Neural Network with Pytorch](https://www.kaggle.com/code/iogbonna/introduction-to-graph-neural-network-with-pytorch)
4. [Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G. (2008). **The graph neural network model**. IEEE transactions on neural networks, 20(1), 61-80.](https://papers.baulab.info/papers/Scarselli-2009.pdf)
5. [Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2018). **How powerful are graph neural networks?**. arXiv preprint arXiv:1810.00826.](https://arxiv.org/pdf/1810.00826)
6. [Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., ... & Sun, M. (2020). **Graph neural networks: A review of methods and applications**. AI open, 1, 57-81.](https://www.sciencedirect.com/science/article/pii/S2666651021000012)
7. [Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Philip, S. Y. (2020). **A comprehensive survey on graph neural networks**. IEEE transactions on neural networks and learning systems, 32(1), 4-24.](https://ieeexplore.ieee.org/ielaam/5962385/9312808/9046288-aam.pdf)
8. [Neural Message Passing for Quantum Chemistry J. Gilmer, S.S. Schoenholz, P.F. Riley, O. Vinyals, G.E. Dahl. Proceedings of the 34th International Conference on Machine Learning, Vol 70, pp. 1263--1272. PMLR. 2017.](http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf)
9. [Relational inductive biases, deep learning, and graph networks P.W. Battaglia, J.B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, R. Pascanu. 2018.](https://www.researchgate.net/profile/Andrea-Tacchetti/publication/325557043_Relational_inductive_biases_deep_learning_and_graph_networks/links/5cd0ad38299bf14d957cca5c/Relational-inductive-biases-deep-learning-and-graph-networks.pdf)
10. [Poulovassilis, A., & Levene, M. (1994). **A nested-graph model for the representation and manipulation of complex objects**. ACM Transactions on Information Systems (TOIS), 12(1), 35-68.](https://dl.acm.org/doi/pdf/10.1145/174608.174610)
11. [Zitnik, M., Agrawal, M., & Leskovec, J. (2018). **Modeling polypharmacy side effects with graph convolutional networks**. Bioinformatics, 34(13), i457-i466.](https://academic.oup.com/bioinformatics/article-pdf/34/13/i457/50316205/bioinformatics_34_13_i457.pdf)
12. [Stocker, S., Csanyi, G., Reuter, K., & Margraf, J. T. (2020). **Machine learning in chemical reaction space**. Nature communications, 11(1), 5505.](https://www.nature.com/articles/s41467-020-19267-x.pdf)