---
layout: post
title: ML | ResNet
categories: ML
tags: [NWP, ML, AI, ResNet]
author: wpsze
date: 2025-01-26 12:38:00
math: true
mathjax: true
mathjax_autoNumber: true
mermaid: true
index_img: https://i.imgur.com/UtmMY3o.png
banner_img: https://i.imgur.com/UtmMY3o.png
---

# 殘差網路概述

**殘差神經網路**（Residual Neural Network，簡稱 **ResNet** ） 是一種深度學習模型，**旨在解決傳統深度神經網路中常見的梯度消失和梯度爆炸問題**。這種網路的核心理念是讓每一層學習與輸入之間的殘差，而非直接學習預期的輸出。這樣的設計使得訓練更深層的模型變得更為有效，並且在增加層數時能保持甚至提高準確率。

**ResNet 透過 殘差學習解決了深度網路的退化問題，讓我們可以訓練出更深的網絡**，這稱得上是深度網絡的一個歷史大突破吧。

## 殘差塊 (Residual Block)

ResNet的基本單位是殘差塊（**Residual Block**），每個殘差塊都包含一個短路連接（**shortcut connection**），這是一條將輸入直接連接到輸出的路徑。這樣，即使在反向傳播過程中出現梯度消失或爆炸，捷徑連接也能確保梯度順利流動，從而促進學習。

ResNet 的核心在於其殘差塊。每個殘差塊包含一個捷徑連接，**這條連接將輸入直接加到輸出上**。這樣做的目的是讓網路能夠學習到恒等映射，即使在其他層出現梯度消失時，捷徑連接仍能確保梯度能夠有效反向傳播。

具體而言，假設 $x$ 是輸入，$F(x)$ 是通過卷積和激活函數計算出的映射，那麼殘差塊的輸出可以表示為：

$$
H(x) = F(x) + x
$$

當網絡學會了恒等映射時，即 $F(x) = 0$，則輸出 $H(x)$ 就等於輸入 $x$。這樣，即使 $F(x)$ 的梯度為0，捷徑連接仍然能夠保證前面層的梯度不會消失

ResNet的作者何愷明也因此摘得CVPR2016最佳論文獎，當然何博士的成就遠不止於此，感興趣的可以去搜一下他後來的輝煌戰績。是解決了深度CNN模型難見訓練的問題，從圖2中可以**14年的VGG才19層**，而**15年的ResNet多達152層**，這在網絡深度如果完全不是一個量級上，所以是第一眼看這張圖的話，一定會覺得ResNet是靠深度取勝。


## 梯度消失的本質

在傳統的深度神經網路中，隨著層數的增加，梯度在反向傳播過程中會逐漸變小，最終導致更新無法進行。這是因為每一層的輸出依賴於前面各層的輸出，當激活函數的導數小於1時，梯度會隨著層數增加而減少，形成梯度消失現象。

### 退化問題（Degradation problem）

實驗發現深度網路出現了**退化問題（Degradation problem）**：網路深度增加時，網路準確度出現飽和，甚至出現下降。這個現象可以在直觀看出來：56層的網路 比 20層網路效果還要差。這不會是過度擬合問題，因為56層網路的訓練誤差同樣高。**我們知道深層網路存在著梯度消失或爆炸的問題**，這使得深度學習模型很難訓練。但是現在已經存在一些技術手段如 BatchNorm 來緩解這個問題。深度網路的退化問題至少說明深度網路不容易訓練。但我們考慮這樣一個事實：現在你有一個淺層網絡，你想透過向上堆積新層來建立深層網絡，一個極端情況是這些增加的層什麼也不學習，僅僅複製淺層網絡的特徵，即這樣新層是恆等映射（Identity mapping）。在這種情況下，深層網路應該至少和淺層網路效能一樣，也不應該出現退化現象。好吧，你不得不承認肯定是目前的訓練方法有問題，才使得深層網路很難去找到一個好的參數。

## 工作原理

### 批歸一化（Batch Normalization）

**批歸一化**是一種正則化技術，用於減少內部協變偏移（Internal Covariate Shift），這是指在訓練過程中，隨著參數更新，層的輸入分佈會發生變化。這會導致訓練速度變慢，並使得模型難以收斂。

#### 工作原理

1. **標準化**：對每一批次的輸入進行標準化，使其均值為0，方差為1。這樣可以使得每層的輸入保持在相似的範圍內。
   
$$
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

其中，$\mu$ 是批次均值，$\sigma^2$ 是批次方差，$\epsilon$ 是一個小常數，用於防止除以零。

2. **縮放和平移**：標準化後，通過學習到的參數 $\gamma$ 和 $\beta$ 對輸出進行縮放和平移，以保持模型的表達能力。

$$
y = \gamma \hat{x} + \beta
$$

這樣做不僅提高了訓練的穩定性，還加速了收斂速度，使得更深層的網絡能夠有效訓練。

### 預激活技術（Pre-activation）

**預激活技術**是對傳統殘差塊結構的一種改進，它將激活函數放置在卷積層之前。這種方法有助於改善梯度流動並提高模型性能。

在傳統的殘差塊中，結構如下：

$$
H(x) = F(x) + x
$$

而在預激活結構中，則改為：

1. **先進行批歸一化和激活**：首先對輸入 $x$ 進行 卷積、批歸一化 (Batch Normalization）。
   
$$
y = F(x)
$$

2. **再進行捷徑連接**：然後將捷徑連接添加到經過處理的輸出上。激活。

$$
H(x) = y + x
$$

這樣做的好處是，在每個殘差塊中都能夠更好地保留信息流動，**使得梯度能夠更有效地反向傳播**。

# "Deep residual learning for image recognition."

- [He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)
  - 作者比較18-layer和34-layer的網路效果，可以看到普通的網路出現退化現象，但ResNet很好的解決了退化問題。


{% gi 5 2-2-1 %}
![](https://i.imgur.com/IRN73Xt.png)
![](https://i.imgur.com/LT2gCRW.png)
![](https://i.imgur.com/YGVdUTF.png)
![](https://i.imgur.com/UtmMY3o.png)
![](https://i.imgur.com/HX57hvz.png)
{% endgi %}

# References

1.[7.6. 残差网络（ResNet）](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html)

