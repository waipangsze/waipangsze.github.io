---
layout: post
title: Physics-informed neural networks (PINN)
categories: [ML]
tags: [PINN, AI]
author: wpsze
date: 2024-06-26 14:00:00
math: true
mathjax: true
mathjax_autoNumber: true
mermaid: true
index_img: 
banner_img: 
---

# PINN

Physics-informed neural networks (PINNs) represent a significant advancement in the integration of machine learning with physical laws, particularly in the context of solving differential equations. They combine traditional neural network approaches with the constraints provided by known physics, enabling more accurate modeling and predictions in various scientific fields.

![PINN](https://upload.wikimedia.org/wikipedia/commons/9/90/Physics-informed_nerural_networks.png){width=500}

# Universal Approximation Theorem

**Definition**  

The theorem states that for **any continuous function** $ f: \mathbb{R}^n \to \mathbb{R} $ defined on a compact subset of $ \mathbb{R}^n $, there exists a neural network $ g $ such that the difference between $ f $ and $ g $ can be made arbitrarily small. Formally, for any $ \epsilon > 0 $, there exists a neural network with a finite number of neurons such that:

$$
| f(x) - g(x) | < \epsilon
$$

for all $ x $ in the domain of interest.

**Historical Background**  

The theorem was independently proven by George Cybenko in 1989, who focused on networks using sigmoid activation functions, and later by Kurt Hornik in 1991, who generalized it to include other activation functions like ReLU (Rectified Linear Unit). 

**Implications**  

It has profound implications for the design and understanding of neural networks:

- **Expressive Power**: It guarantees that neural networks can model complex relationships in data, making them suitable for tasks like image classification, function approximation, and more.
- **Architecture Design**: The theorem informs users about the necessary architecture (e.g., number of layers and neurons) required to achieve desired approximations.

**Limitations**  

While the UAT provides a theoretical foundation, it does not address practical aspects such as:

- **Generalization**: The ability of a network to perform well on unseen data is not guaranteed by the theorem alone. Techniques like regularization and cross-validation are necessary to improve generalization.
- **Computational Feasibility**: The theorem does not specify how to construct the network or find the optimal parameters effectively. In practice, training methods like backpropagation are used, but they may not always reach the global optimum.

**Reading**

- [The Universal Approximation Theorem](https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/)
- [Expressivity and Universal Approximation Theorems Part 1](https://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf)

## For function alone,

- Chen, T., & Chen, H. (1993). Approximations of continuous functionals by neural networks with application to dynamic systems. IEEE Transactions on Neural networks, 4(6), 910-918.

Theorem 2: Suppose that $U$ is a compact set in $C[a,b]$, $f$ is a continuous functional defined on $U$, and $\sigma (x)$ is a bounded generalized sigmoidal function, then for any $\epsilon > 0$, there exist $m+1$ points $ a = x_0 <...< x_m = b$, a positive integer $N$ and constants $c_{ij}, \theta_{ij}, \xi_{ij}$, $i=1,...,N$, $j=0,1,...,m$, such that

$$
| f(u) - \sum_{i=1}^N c_i g ( \sum_{j=0}^m \xi_{ij} u(x_{j}) + \theta_i) | < \epsilon, \qquad \forall u \in U 
$$

or

$$
| f(u) - \sum_{i=1}^N c_i g (\mathbf{\omega_j} \cdot \mathbf{x} + \theta_i) | < \epsilon, \qquad \forall u \in U
$$

## For operator,

- Chen, T., & Chen, H. (1995). Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. IEEE transactions on neural networks, 6(4), 911-917.

Theorem 5: Suppose that $g\in (TW)$, $X$ is a Banach Space, $K_1 X, K_2 R^n$ are two compact sets in $X$ and $R^n$ respectively, $V$ is a compace set in $C(K_1)$, $G$ is a nonlinear continuous operator, which maps $V$ into $C(K_2)$, then for any $\epsilon >0 $, there are positive integers $M, N, m$ constants $c_i^k, \zeta_k, \xi_{ij}^k \in R^n$, $x_j \in K_1, i=1,...,M, k=1,...,N, j=1,...,m$, such that

$$
| G(u)(y) - \sum_{k=1}^N \sum_{i=1}^M c_i^k g( \sum_{j=1}^m \xi_{ij} u(x_{j}) + \theta_i^k) g(\omega_k \cdot y + \xi_k)| < \epsilon, \qquad \forall u \in U 
$$

# Definition and Functionality

PINNs are a type of **universal function approximator** that incorporates physical laws directly into the training process of neural networks. This is achieved by embedding governing equations, typically expressed as partial differential equations (PDEs), into the loss function of the network. By doing so, PINNs ensure that the solutions generated are consistent with these physical laws, which enhances the robustness and accuracy of the model even when data availability is limited

**Training Mechanism**  

The training of a PINN involves two main components:

1. **Data Loss**: The standard supervised learning approach minimizes the mean squared error (MSE) between **predicted outputs and actual data**.
2. **Physics Loss**: An additional term is introduced to the loss function that quantifies **how well the predicted outputs satisfy the governing physical equations**. This is done by computing gradients (**using automatic differentiation**) at sampled points in space and time, allowing for the evaluation of residuals from the PDEs.

This dual approach allows PINNs to **leverage both empirical data and theoretical knowledge**, making them particularly effective for problems where data is scarce or noisy.

## Challenges and Future Directions

Despite their advantages, PINNs also face challenges:

- **Computational Cost**: Training PINNs can be computationally intensive, especially when dealing with **complex geometries** or **high-dimensional problems**.
- **Generalization Limitations**: Regular PINNs typically require **retraining for different geometries or boundary conditions**, which can limit their efficiency in practical applications.

- Steve Brunton

<iframe width="560" height="315" src="https://www.youtube.com/embed/-zrY7P2dVC4?si=u64XUih1hSEUEv6o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

# References

1. Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378, 686-707.
2. Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., & Yang, L. (2021). Physics-informed machine learning. Nature Reviews Physics, 3(6), 422-440.
3. Lu, L., Jin, P., Pang, G., Zhang, Z., & Karniadakis, G. E. (2021). Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature machine intelligence, 3(3), 218-229.



